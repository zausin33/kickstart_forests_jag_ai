{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Machine Learning Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-27T14:56:24.250187500Z",
     "start_time": "2024-01-27T14:56:22.889182800Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import optuna\n",
    "import pandas as pd\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OrdinalEncoder, OneHotEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=UserWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-27T14:56:24.711417600Z",
     "start_time": "2024-01-27T14:56:24.252211400Z"
    }
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('../data/RtmSimulation_kickstart.csv', index_col= 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Descriptive Statistics"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 1000 entries, 1 to 1000\n",
      "Columns: 2114 entries, lai to w2500\n",
      "dtypes: float64(2113), object(1)\n",
      "memory usage: 16.1+ MB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-27T14:56:25.168425Z",
     "start_time": "2024-01-27T14:56:25.063617Z"
    }
   },
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "data.describe()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "data.isnull().sum()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Descriptive Visualization"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "columns_with_missing_values = data.columns[data.isnull().any()]\n",
    "sns.heatmap(data[columns_with_missing_values].isnull(), cbar = False)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data Quality Summary\n",
    "* The dataset contains 1000 rows.\n",
    "* The column 'lai' is the target variable.\n",
    "* The feature columns can be divided into three groups:\n",
    "    * Numerical feature wetness\n",
    "    * Categorical feature treeSpecies\n",
    "    * 10 Numerical features of Sentinel-2A wavelengths\n",
    "    * 2101 Numerical features of wavelengths\n",
    "* 65 rows with at least one null value. Null values are only present in the columns 'Sentinel_2A_704.1', 'Sentinel_2A_740.5', 'Sentinel_2A_782.8', 'w469', 'w470', 'w471', 'w473', 'w474'"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define target and features"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Identify categorical and numerical columns\n",
    "categorical_cols = [col for col in data.columns if data[col].dtype == 'object']\n",
    "numerical_cols = [col for col in data.columns if col not in categorical_cols + ['id', 'lai']]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-27T14:29:49.958010200Z",
     "start_time": "2024-01-27T14:29:49.924997300Z"
    }
   },
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "X = data.drop(['lai'], axis=1)\n",
    "y = data['lai']"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-27T14:29:51.388943500Z",
     "start_time": "2024-01-27T14:29:51.369946300Z"
    }
   },
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Train-Test Split\n",
    "Feature engineering steps are applied separately to the train test and validation sets to avoid data leakage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-27T14:29:53.537326600Z",
     "start_time": "2024-01-27T14:29:53.507196500Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=42)\n",
    "X_test, X_val, y_test, y_val = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "sns.kdeplot(y_train, label = 'y_train')\n",
    "sns.kdeplot(y_val, label = 'y_val')\n",
    "sns.kdeplot(y_test, label = 'y_test')\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Feature Engineering\n",
    "\n",
    "* We fill missing values with the median of each column.\n",
    "* Categories are encoded with OrdinalEncoder and then passed to LightGBM as categorical features. LightGBM will handle the encoding internally.\n",
    "* Numerical features are scaled with StandardScaler."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Fill missing values"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "X_train[numerical_cols] = X_train[numerical_cols].fillna(X_train[numerical_cols].median())\n",
    "X_val[numerical_cols] = X_val[numerical_cols].fillna(X_train[numerical_cols].median())\n",
    "X_test[numerical_cols] = X_test[numerical_cols].fillna(X_train[numerical_cols].median())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-27T14:30:01.905276600Z",
     "start_time": "2024-01-27T14:30:00.310728700Z"
    }
   },
   "execution_count": 11
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Encode categorical features"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "ordinal_encoder = OrdinalEncoder()\n",
    "X_train[categorical_cols] = ordinal_encoder.fit_transform(X_train[categorical_cols])\n",
    "X_val[categorical_cols] = ordinal_encoder.transform(X_val[categorical_cols])\n",
    "X_test[categorical_cols] = ordinal_encoder.transform(X_test[categorical_cols])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Scale numerical features"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "scl = StandardScaler()\n",
    "X_train[numerical_cols] = scl.fit_transform(X_train[numerical_cols])\n",
    "X_val[numerical_cols] = scl.transform(X_val[numerical_cols])\n",
    "X_test[numerical_cols] = scl.transform(X_test[numerical_cols])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-27T14:30:03.948619100Z",
     "start_time": "2024-01-27T14:30:03.338226Z"
    }
   },
   "execution_count": 12
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### LightGBM"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-27T14:30:05.223956900Z",
     "start_time": "2024-01-27T14:30:05.202939800Z"
    }
   },
   "outputs": [],
   "source": [
    "gbm_model = LGBMRegressor(random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "gbm_model.fit(X_train, y_train, categorical_feature=set(categorical_cols))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train MSE: 0.0021111604630675903 Train r2: 0.9994071322907638\n",
      "Validation MSE: 0.40356709369591093 Validation r2: 0.8889391624860117\n",
      "Test MSE: 0.45539207185638764 Test r2: 0.8948183482390067\n"
     ]
    }
   ],
   "source": [
    "y_pred_train = gbm_model.predict(X_train)\n",
    "mse = mean_squared_error(y_train, y_pred_train)\n",
    "\n",
    "print(\"Train MSE:\", mse, \"Train r2:\", r2_score(y_train, y_pred_train))\n",
    "\n",
    "y_pred_val = gbm_model.predict(X_val)\n",
    "mse = mean_squared_error(y_val, y_pred_val)\n",
    "\n",
    "print(\"Validation MSE:\", mse, \"Validation r2:\", r2_score(y_val, y_pred_val))\n",
    "\n",
    "y_pred_test = gbm_model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred_test)\n",
    "\n",
    "print(\"Test MSE:\", mse, \"Test r2:\", r2_score(y_test, y_pred_test))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-27T14:31:03.479302900Z",
     "start_time": "2024-01-27T14:31:03.249818Z"
    }
   },
   "execution_count": 15
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Hyperparameter Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-27T14:32:03.235503200Z",
     "start_time": "2024-01-27T14:32:03.211502200Z"
    }
   },
   "outputs": [],
   "source": [
    "def param_bounds(trial: optuna.Trial):\n",
    "\n",
    "    return {\n",
    "        # Sample an integer between 10 and 100\n",
    "        \"n_estimators\": trial.suggest_categorical(\"n_estimators\", [50, 100, 200, 300, 400, 500, 700, 1000]),\n",
    "        \"num_leaves\": trial.suggest_int(\"num_leaves\", 10, 100),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 10, 100),\n",
    "        # Sample a categorical value from the list provided\n",
    "        \"objective\": trial.suggest_categorical(\n",
    "            \"objective\", [\"regression\", \"regression_l1\", \"huber\"]\n",
    "        ),\n",
    "        \"random_state\": [42],\n",
    "        # Sample from a uniform distribution between 0.3 and 1.0\n",
    "        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.3, 1.0),\n",
    "        # Sample from a uniform distribution between 0 and 10\n",
    "        \"reg_alpha\": trial.suggest_float(\"reg_alpha\", 0, 100),\n",
    "        # Sample from a uniform distribution between 0 and 10\n",
    "        \"reg_lambda\": trial.suggest_float(\"reg_lambda\", 0, 100),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def objective(trial: optuna.Trial):\n",
    "    gbm_model = LGBMRegressor()\n",
    "    params = param_bounds(trial)\n",
    "    gbm_model.set_params(**params)\n",
    "\n",
    "    gbm_model.fit(X_train, y_train, categorical_feature=set(categorical_cols))\n",
    "\n",
    "    return gbm_model.score(X_val, y_val)\n",
    "\n",
    "    \n",
    "sampler = optuna.samplers.TPESampler(n_startup_trials=10, seed=42)\n",
    "# Create a study\n",
    "study = optuna.create_study(direction=\"maximize\", sampler=sampler)\n",
    "# Start the optimization run\n",
    "study.optimize(objective, n_trials=50, show_progress_bar=True)\n",
    "\n",
    "fig = optuna.visualization.plot_param_importances(study)\n",
    "fig.show()\n",
    "\n",
    "bo_search_trials = study.trials_dataframe()\n",
    "best_params = study.best_params\n",
    "best_score = study.best_value\n",
    "print(bo_search_trials.sort_values(\"value\").head())\n",
    "print(f\"Best parameters: {best_params}\")\n",
    "print(f\"Best score: {best_score}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Train with best parameters"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "gbm_model = LGBMRegressor(random_state=42, **best_params)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "gbm_model.fit(X_train, y_train, categorical_feature=set(categorical_cols))\n",
    "end = time.time()\n",
    "print(f\"Training time: {end - start} seconds\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "y_pred_train = gbm_model.predict(X_train)\n",
    "end = time.time()\n",
    "mse = mean_squared_error(y_train, y_pred_train)\n",
    "print(\"Train MSE:\", mse, \"Train r2:\", r2_score(y_train, y_pred_train), f\"Prediction time: {end - start} seconds\")\n",
    "\n",
    "start = time.time()\n",
    "y_pred_val = gbm_model.predict(X_val)\n",
    "end = time.time()\n",
    "mse = mean_squared_error(y_val, y_pred_val)\n",
    "print(\"Validation MSE:\", mse, \"Validation r2:\", r2_score(y_val, y_pred_val), f\"Prediction time: {end - start} seconds\")\n",
    "\n",
    "start = time.time()\n",
    "y_pred_test = gbm_model.predict(X_test)\n",
    "end = time.time()\n",
    "mse = mean_squared_error(y_test, y_pred_test)\n",
    "print(\"Test MSE:\", mse, \"Test r2:\", r2_score(y_test, y_pred_test), f\"Prediction time: {end - start} seconds\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross validation\n",
    "We concatenate the train and validation sets to perform cross-validation on the whole dataset."
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "gbm_model = LGBMRegressor(random_state=42, **best_params)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-27T14:33:51.750553100Z",
     "start_time": "2024-01-27T14:33:51.729554600Z"
    }
   },
   "execution_count": 19
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "X_train_cross_val, y_train_cross_val = pd.concat([X_train, X_val]), pd.concat([y_train, y_val])\n",
    "\n",
    "# Define the cross-validation strategy\n",
    "cv_strategy = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Perform cross-validation for R² score\n",
    "r2_scores = cross_val_score(gbm_model, X_train_cross_val, y_train_cross_val, cv=cv_strategy, fit_params={'categorical_feature': set(categorical_cols)})\n",
    "\n",
    "# Calculate mean and standard deviation of R² scores\n",
    "r2_mean = np.mean(r2_scores)\n",
    "r2_std = np.std(r2_scores)\n",
    "\n",
    "# Perform cross-validation for MSE\n",
    "mse_scores = cross_val_score(gbm_model, X_train, y_train, scoring='neg_mean_squared_error', cv=cv_strategy, fit_params={'categorical_feature': set(categorical_cols)})\n",
    "\n",
    "# Calculate mean and standard deviation of MSE scores\n",
    "mse_mean = np.mean(-mse_scores)  # Negate the scores back to positive\n",
    "mse_std = np.std(-mse_scores)    # Negate the scores back to positive\n",
    "\n",
    "print(f\"Mean cross-validation R²: {r2_mean:.3f} +/- {r2_std:.3f}\")\n",
    "print(f\"Mean cross-validation MSE: {mse_mean:.3f} +/- {mse_std:.3f}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Final evaluation\n",
    "We train the model on the whole dataset and evaluate it on the test set, to get the final performance metrics."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "gbm_model.fit(X_train_cross_val, y_train_cross_val, categorical_feature=set(categorical_cols))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "y_pred_test = gbm_model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred_test)\n",
    "\n",
    "print(\"Test MSE:\", mse, \"Test r2:\", r2_score(y_test, y_pred_test))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Explainability\n",
    "Gradient boosting models have a feature_importances_ attribute that can be used to get the relative importance of each feature.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "feat_df = pd.DataFrame(\n",
    "    {\n",
    "        \"feature\": X_train.columns,\n",
    "        \"importance\": gbm_model.feature_importances_.ravel(),\n",
    "    }\n",
    ")\n",
    "\n",
    "feat_df[\"_abs_imp\"] = np.abs(feat_df.importance)\n",
    "feat_df = feat_df.sort_values(\"_abs_imp\", ascending=False).drop(\n",
    "    columns=\"_abs_imp\"\n",
    ")\n",
    "\n",
    "feat_df = feat_df.sort_values(by=\"importance\", ascending=False).head(15)\n",
    "feat_df.plot(x=\"feature\", y=\"importance\", kind=\"bar\", color=\"blue\", )"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
